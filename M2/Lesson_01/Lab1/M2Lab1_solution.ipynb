{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "lastEditStatus": {
   "notebookId": "cjhsfyjzfjnlqlor3gvm",
   "authorId": "1380853937679",
   "authorName": "DATAPROFESSOR",
   "authorEmail": "chanin.nanta@gmail.com",
   "sessionId": "e6c4c58f-821a-4353-b203-998a8e857ace",
   "lastEditTime": 1763094927755
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41dff3df-18d2-44b7-a960-8a75518e198b",
   "metadata": {
    "name": "cell1"
   },
   "source": [
    "# Lab: Ingesting and Analyzing Avalanche Shipping Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da5e94-56e0-4027-a119-dc2fb13a88cc",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "**Objective:**  \nYou’ll use Snowflake and Python to ingest a CSV file containing Avalanche shipping logs. Then you will clean the data, and extract useful insights that can inform the next stage of your GenAI prototype."
  },
  {
   "cell_type": "markdown",
   "id": "daa4cd7c-a7f9-4192-91b9-6b0c3d5a7baf",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "**Prerequisite:**  \nMake sure that you've already gone through the following prerequisite notebooks of Module 2 before proceeding with this lab's notebook:\n- [M2L1V3.ipynb](https://github.com/dataprofessor/fast-prototyping-of-genai-apps-with-streamlit/blob/main/M2/Lesson_01/M2L1V3.ipynb)\n- [M2L1V4.ipynb](https://github.com/dataprofessor/fast-prototyping-of-genai-apps-with-streamlit/blob/main/M2/Lesson_01/M2L1V4.ipynb)\n- [M2L1V5.ipynb](https://github.com/dataprofessor/fast-prototyping-of-genai-apps-with-streamlit/blob/main/M2/Lesson_01/M2L1V5.ipynb)\n- [M2L1V6.ipynb](https://github.com/dataprofessor/fast-prototyping-of-genai-apps-with-streamlit/blob/main/M2/Lesson_01/M2L1V6.ipynb)"
  },
  {
   "cell_type": "markdown",
   "id": "d8aaa278-246b-4b73-b585-e3fa900e8d73",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "**Story Context:**  \nYou just got your hands on internal shipping logs from Avalanche's distribution warehouse. These logs contain important operational data — delivery errors, shipping times, product IDs, and destinations.\n\nYour job is to:\n1. Upload the CSV file into Snowflake.\n2. Clean and explore the data.\n3. Save the data to a table in the database."
  },
  {
   "cell_type": "markdown",
   "id": "e9c711bc-f6f9-4a57-8165-d4208194a809",
   "metadata": {
    "name": "cell5"
   },
   "source": [
    "## ✅ Step 1: Upload the file to Snowflake Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce3c604-5160-494e-b9b0-a8d11e7dd9bf",
   "metadata": {
    "name": "cell6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "\n",
    "# Read the shipping log CSV file from the Snowflake stage with headers\n",
    "shipping_df = session.read.options({\n",
    "    \"inferSchema\": True,\n",
    "    \"header\": True\n",
    "}).csv(\"@AVALANCHE_STAGE/shipping_logs.csv\")\n",
    "# Preview the data\n",
    "shipping_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed55a8c9-3a22-4b01-a991-d7f40bd6f608",
   "metadata": {
    "name": "cell7"
   },
   "source": [
    "## ✅ Step 2: Clean and explore the data\n",
    "\n",
    "Start by renaming the columns. Looking at the dataframe preview above, you can see that the column names have quotes around them. That's because snowpark loads column names exactly as they appear in the file—including quotes, capital letters, and spaces. The easy way to fix that is to just update them with an alias that renames the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92aada4-b40d-4fbb-916a-02892bc18f06",
   "metadata": {
    "name": "cell8",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.functions import col\n",
    "\n",
    "# Rename columns explicitly (remove any extra characters or spaces)\n",
    "shipping_df = shipping_df.select(\n",
    "    col('\"Order ID\"').alias(\"Order_ID\"),\n",
    "    col('\"Shipping Date\"').alias(\"Shipping_Date\"),\n",
    "    col('\"Carrier\"').alias(\"Carrier\"),\n",
    "    col('\"Tracking Number\"').alias(\"Tracking_Number\"),\n",
    "    col('\"Latitude\"').alias(\"Latitude\"),\n",
    "    col('\"Longitude\"').alias(\"Longitude\"),\n",
    "    col('\"Status\"').alias(\"Status\"),\n",
    "    col('\"Delivery Days\"').alias(\"Delivery_Days\"),\n",
    "    col('\"Late\"').alias(\"Late\"),\n",
    "    col('\"Region\"').alias(\"Region\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a37555-1e4a-4567-9bf3-c76cd064b19f",
   "metadata": {
    "name": "cell9"
   },
   "source": [
    "Now you can count how many shipments each carrier made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41f14a-d1bc-40ef-b591-3d11a6931b0b",
   "metadata": {
    "name": "cell10",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Count how many shipments each carrier made\n",
    "shipping_df.group_by(\"Carrier\").count().show()\n",
    "\n",
    "# Count late shipments\n",
    "shipping_df.filter(shipping_df[\"Late\"] == True).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32d3b4-cc8d-4f08-83d8-a54714957b3c",
   "metadata": {
    "name": "cell11"
   },
   "source": [
    "Since this dataset is small, you can easily convert it to pandas and use some of the built-in functions to explore it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7fa744-ce9b-46f8-a3a1-6e368c600fcf",
   "metadata": {
    "name": "cell12",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "pandas_df = shipping_df.to_pandas()\n",
    "pandas_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b14503-f538-4c6d-a847-aef959609b7e",
   "metadata": {
    "name": "cell13"
   },
   "source": [
    "## ✅ Step 3: Save the data to a table in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6be53-fd40-40d3-baad-396e42896667",
   "metadata": {
    "name": "cell14",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "shipping_df.write.save_as_table(\"shipping_logs\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cd7d25-eff9-4b18-a0d5-66ecce8716f2",
   "metadata": {
    "name": "cell15"
   },
   "source": [
    "## ✅ Challenge Extension (Optional)\n",
    "Try building a quick prototype in Streamlit that lets a user:\n",
    "- Search shipping logs by location or keyword\n",
    "- Display the top locations with shipping issues"
   ]
  },
  {
   "cell_type": "code",
   "id": "148674d8-acc0-429d-b1dd-d8c8d04d7c4b",
   "metadata": {
    "language": "sql",
    "name": "cell16"
   },
   "outputs": [],
   "source": "select * FROM shipping_logs;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7837d333-6520-423f-aea9-041449d8c9db",
   "metadata": {
    "language": "sql",
    "name": "cell17",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}